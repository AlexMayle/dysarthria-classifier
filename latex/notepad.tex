 Barring the first experiment, where we compare against a baseline model, we perform ten fold validations to evaluate the models. We first randomly select nine speakers as our validation set. The remaining sixty speakers are randomly split into ten folds of six. There is a non-negligible variance between each cross validation, so we perform three and provide the average metrics

These were first grouped and shuffled, then split into a training, validation, and test set with a ratio of 2:1:1. Each data point represents a spoken Mandarin character recorded under the supervision of a medical professional. 

A value of $1.25\%$ was used for the early stopping parameter $a$. The LSTM models use a hidden state of size 200, resulting in a total of 800 parameters. These are initialized to a truncated normal distribution with $\mu = 0$ and $\sigma = 0.5$. Values generated beyond two standard deviations are discarded and re-picked. The logistic regression layer is also of size 200, except in the case of the bidirectional LSTM, where it is of size 400 to accommodate the concatenation of the two passes' outputs.

LSTM-2's $88.7$\% accuracy and 8.2\% false negative rate constitute a promising attempt at classifying Dysarthria among both afflicted and healthy speakers, but it is not medically reliable. Of course, in such a setting, an accuracy of ~$99.9$\% is desired. While this is certainly hard to achieve and a slightly lower performance may be acceptable in practice, the results of these experiments are not. Specifically, the rate of false-negatives must be decreased by an order of magnitude. 

We also observed that each network is not prone to over-fitting. While we implemented early stopping, it never triggered training to end early. Furthermore, neither $L!$, $L2$, or dropout regularization significantly improved performance, with the latter being detrimental to LSTM-1 and Bi-LSTM-1. 

Instead of a bottleneck in the network, it may be the data that prevents an increase in performance. For example, a medical doctor would never diagnose someone with Dysarthria based off of how a patient pronounces a single character. If the input to the network was instead a series of speech from the the same speaker, the network may be given a better opportunity to discern the condition of the speaker. Moreover, the effects of Dysarthria may not present themselves to the same degree across different character pronunciations. If the input to the network is one with little variation between healthy and afflicted individuals, the network will be at a disadvantage to make an accurate diagnosis. 
